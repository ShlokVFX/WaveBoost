# WaveBoost - Minimal Flash Attention CUDA Implementation

## Summary

WaveBoost is my personal repository to experiment with inference-time optimizations.
I implemented individual CUDA kernels for LLM inference.

---

## ðŸ“š References

### Flash Attention Papers
- **Flash Attention v1**: [Dao et al., 2022](https://arxiv.org/abs/2205.14135) - Fast and Memory-Efficient Exact Attention with IO-Awareness
- **Flash Attention v2**: [Dao, 2023](https://arxiv.org/abs/2307.08691) - Faster Attention with Better Parallelism and Work Partitioning

### CUDA Optimization Resources
- [NVIDIA CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [NVIDIA Cutlass - CUDA Templates](https://github.com/NVIDIA/cutlass)
- [Programming Massively Parallel Processors](https://www.elsevier.com/books/programming-massively-parallel-processors/kirk/978-0-323-91231-4)
---